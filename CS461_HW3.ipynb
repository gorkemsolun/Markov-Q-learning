{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "JOVOX-VhDKvS",
   "metadata": {
    "id": "JOVOX-VhDKvS"
   },
   "source": [
    "# CS461 Homework Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7b628",
   "metadata": {
    "id": "02a7b628"
   },
   "source": [
    "# Part 1: Solving Cliff Walking with Value Iteration and Policy Iteration\n",
    "In this part, you will implement **Value Iteration** and **Policy Iteration** methods to solve the `CliffWalking` environment from the Gymnasium library. The goal is to find an optimal policy that minimizes the total cost while avoiding the cliff.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand and implement core MDP methods (Value Iteration and Policy Iteration).\n",
    "- Visualize and analyze the convergence of state values and policies.\n",
    "- Compare the performance of both methods in terms of iterations and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7682044",
   "metadata": {
    "id": "c7682044"
   },
   "source": [
    "## Setup\n",
    "### Install Required Libraries\n",
    "To begin, ensure you have the required packages installed. Run the following cells to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4vhCexoicqFr",
   "metadata": {
    "id": "4vhCexoicqFr"
   },
   "outputs": [],
   "source": [
    "!pip install swig --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f06f8",
   "metadata": {
    "id": "5b5f06f8"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[all] matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3644ef",
   "metadata": {
    "id": "1b3644ef"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.envs.toy_text import CliffWalkingEnv\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbb723",
   "metadata": {
    "id": "30bbb723"
   },
   "source": [
    "## Environment\n",
    "The `CliffWalkingEnv` is a 4x12 grid-world environment. The agent starts at the top-left corner and aims to reach the top-right corner. Falling into the cliff results in a significant penalty. The environment has four possible actions:\n",
    "- **0**: Move up\n",
    "- **1**: Move right\n",
    "- **2**: Move down\n",
    "- **3**: Move left\n",
    "\n",
    "### Example Render:\n",
    "S = Start, G = Goal, C = Cliff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y9yA6Si8iGCd",
   "metadata": {
    "id": "Y9yA6Si8iGCd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "\n",
    "class CliffEnvWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.start_state = (3, 0)  # Assumes a fixed start state\n",
    "        self.terminal_state = (\n",
    "            self.env.shape[0] - 1,\n",
    "            self.env.shape[1] - 1,\n",
    "        )  # Bottom-right corner\n",
    "        self.start_state_index = np.ravel_multi_index(self.start_state, self.env.shape)\n",
    "        self.terminal_state_index = np.ravel_multi_index(\n",
    "            self.terminal_state, self.env.shape\n",
    "        )\n",
    "\n",
    "        # Expose `P` as a direct attribute\n",
    "        self.P = env.P\n",
    "\n",
    "        # Correct the transition probabilities\n",
    "        self.correct_transitions()\n",
    "\n",
    "    def correct_transitions(self):\n",
    "        \"\"\"Corrects the environment's transition probabilities for cliffs and terminal states.\"\"\"\n",
    "        for state in range(self.env.nS):\n",
    "            position = np.unravel_index(state, self.env.shape)\n",
    "\n",
    "            for action in range(self.env.nA):\n",
    "                transitions = self.env.P[state][action]\n",
    "\n",
    "                # If the state is a cliff, reset to the start state with penalty\n",
    "                if self.env._cliff[position]:\n",
    "                    self.env.P[state][action] = [\n",
    "                        (1.0, self.start_state_index, -100, False)\n",
    "                    ]\n",
    "                    continue\n",
    "\n",
    "                # If the state is terminal, transitions should only lead to itself\n",
    "                if position == self.terminal_state:\n",
    "                    self.env.P[state][action] = [(1.0, state, 0, True)]\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, keep the original transition logic\n",
    "                new_transitions = []\n",
    "                for prob, next_state, reward, done in transitions:\n",
    "                    next_position = np.unravel_index(next_state, self.env.shape)\n",
    "                    # Ensure terminal state is correctly handled\n",
    "                    if next_position == self.terminal_state:\n",
    "                        new_transitions.append((prob, next_state, reward, True))\n",
    "                    else:\n",
    "                        new_transitions.append((prob, next_state, reward, False))\n",
    "                self.env.P[state][action] = new_transitions\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Override step to ensure consistency with wrapped transitions.\"\"\"\n",
    "        state, reward, done, truncated, info = super().step(action)\n",
    "\n",
    "        # If the agent steps into a cliff, reset it to the start state\n",
    "        position = np.unravel_index(self.env.s, self.env.shape)\n",
    "        if self.env._cliff[position]:\n",
    "            self.env.s = self.start_state_index  # Reset to start state\n",
    "            return self.env.s, reward, done, truncated, info\n",
    "\n",
    "        return state, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea1c5",
   "metadata": {
    "id": "049ea1c5"
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv(render_mode=\"rgb_array\")\n",
    "wrapped_env = CliffEnvWrapper(env)\n",
    "\n",
    "# Use the wrapped environment\n",
    "observation, info = wrapped_env.reset()\n",
    "wrapped_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a50169",
   "metadata": {
    "id": "b7a50169"
   },
   "source": [
    "## Task 1: Implement Value Iteration\n",
    "Complete the function `value_iteration` below. This method computes the optimal state-value function and policy by iteratively updating state values using the Bellman optimality equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c2c2e",
   "metadata": {
    "id": "487c2c2e"
   },
   "outputs": [],
   "source": [
    "# Value Iteration function skeleton\n",
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to compute the optimal value function and policy.\n",
    "    Args:\n",
    "        env: The CliffWalking environment.\n",
    "        gamma: Discount factor.\n",
    "        theta: Threshold for convergence.\n",
    "    Returns:\n",
    "        V: Optimal state-value function.\n",
    "        policy: Optimal policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    # Repeat until Δ < θ\n",
    "    value_count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # DONE: Update state values using Bellman optimality equation\n",
    "        # V(s) = max_a Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = V[state]\n",
    "            q_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                q_value = 0\n",
    "                # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # P(s' | s, a) * [R(s, a, s') + gamma * V(s')]\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "                q_values.append(q_value)\n",
    "            # V(s) = max_a Q(s, a)\n",
    "            V[state] = max(q_values)\n",
    "            # delta = max(delta, |v - V(s)|)\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "\n",
    "        value_count += 1\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    # DONE: Derive optimal policy from the value function\n",
    "    # π(s) = argmax_a Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = []\n",
    "        # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                # P(s' | s, a) * [R(s, a, s') + gamma * V(s')]\n",
    "                q_value += prob * (reward + gamma * V[next_state])\n",
    "            q_values.append(q_value)\n",
    "        # π(s) = argmax_a Q(s, a)\n",
    "        policy[state] = np.argmax(q_values)\n",
    "\n",
    "    print(f\"Value Iteration converged after {value_count} iterations.\")\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03404f5",
   "metadata": {
    "id": "e03404f5"
   },
   "source": [
    "## Task 2: Implement Policy Iteration\n",
    "Complete the function `policy_iteration` below. This method alternates between policy evaluation and policy improvement until the policy converges to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99b06d",
   "metadata": {
    "id": "2a99b06d"
   },
   "outputs": [],
   "source": [
    "# Policy Iteration function skeleton\n",
    "def policy_iteration(env, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Performs Policy Iteration to compute the optimal value function and policy.\n",
    "    Args:\n",
    "        env: The CliffWalking environment.\n",
    "        gamma: Discount factor.\n",
    "    Returns:\n",
    "        V: Optimal state-value function.\n",
    "        policy: Optimal policy.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(\n",
    "        env.observation_space.n, dtype=int\n",
    "    )  # Initialize a random deterministic policy\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    policy_count = 0\n",
    "\n",
    "    while True:\n",
    "        # DONE: Perform policy evaluation and policy improvement\n",
    "        # Policy Evaluation\n",
    "        # Repeat until Δ < θ\n",
    "        while True:\n",
    "            delta = 0\n",
    "            # V(s) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "            for state in range(env.observation_space.n):\n",
    "                v = V[state]\n",
    "                q_value = 0\n",
    "                action = policy[state]  # π(s)\n",
    "                # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]] where a = π(s)\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # P(s' | s, a) * [R(s, a, s') + gamma * V(s')]\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "                # V(s) = Q(s, a) where a = π(s)\n",
    "                V[state] = q_value\n",
    "                # delta = max(delta, |v - V(s)|)\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "\n",
    "            policy_count += 1\n",
    "\n",
    "            # If the value function converged, break out of the loop\n",
    "            if delta < 1e-6:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for state in range(env.observation_space.n):\n",
    "            old_action = policy[state]\n",
    "            q_values = []\n",
    "            # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "            for action in range(env.action_space.n):\n",
    "                q_value = 0\n",
    "                # Q(s, a) = Σ [P(s' | s, a) * [R(s, a, s') + gamma * V(s')]]\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # P(s' | s, a) * [R(s, a, s') + gamma * V(s')]\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "                q_values.append(q_value)\n",
    "            # π(s) = argmax_a Q(s, a)\n",
    "            policy[state] = np.argmax(q_values)\n",
    "\n",
    "            # If the policy is not stable, break out of the loop\n",
    "            if old_action != policy[state]:\n",
    "                policy_stable = False\n",
    "\n",
    "        # DONE: Check for policy convergence\n",
    "        # If the policy is stable, break out of the loop\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    print(f\"Policy Iteration converged after {policy_count} iterations.\")\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603906f",
   "metadata": {
    "id": "3603906f"
   },
   "source": [
    "## Visualization\n",
    "To help you debug and understand the algorithms, use the following function to visualize the state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631e053",
   "metadata": {
    "id": "6631e053"
   },
   "outputs": [],
   "source": [
    "def visualize_values(values, grid_size=(4, 12)):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    grid_values = np.array(values).reshape(grid_size)\n",
    "    plt.imshow(grid_values, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    for (i, j), val in np.ndenumerate(grid_values):\n",
    "        plt.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "    plt.title(\"State Values\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e724d",
   "metadata": {
    "id": "279e724d"
   },
   "outputs": [],
   "source": [
    "# Test the solutions and visualize results\n",
    "# Value Iteration\n",
    "\n",
    "wrapped_env.reset()\n",
    "V_vi, policy_vi = value_iteration(wrapped_env)\n",
    "visualize_values(V_vi)\n",
    "\n",
    "\n",
    "# Policy Iteration\n",
    "V_pi, policy_pi = policy_iteration(env)\n",
    "visualize_values(V_pi)\n",
    "\n",
    "# Try with different gamma values\n",
    "gamma_values = [0.1, 0.5, 0.9, 0.99]\n",
    "V_values = []\n",
    "policy_values = []\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Gamma: {gamma}\")\n",
    "    wrapped_env.reset()\n",
    "    V, policy = value_iteration(wrapped_env, gamma=gamma)\n",
    "    V_values.append(V)\n",
    "    policy_values.append(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e80f7",
   "metadata": {
    "id": "8b1e80f7"
   },
   "source": [
    "## Task 3: Analysis\n",
    "\n",
    "Briefly answer the questions below. You may write your answers after \"Answer:\".\n",
    "\n",
    "1. Compare the number of iterations and computation time for Value Iteration and Policy Iteration.\n",
    "\n",
    "Answer: \n",
    "Value Iteration: Converged after 15 iterations, indicating a faster convergence rate.\n",
    "Policy Iteration: Converged after 1404 iterations, showing significantly more iterations and likely higher computation time compared to Value Iteration.\n",
    "This suggests that Value Iteration converges faster in this specific Cliff Walking environment, making it more efficient in terms of iterations and computation time.\n",
    "\n",
    "2. Visualize the state values and return the optimal policy for both methods and discuss any differences.\n",
    "\n",
    "Answer:\n",
    "The state values are visualized in the heatmaps provided:\n",
    "\n",
    "Both methods show similar state values, with the lowest values along the bottom row where the cliff penalty is incurred.\n",
    "Optimal Policies: Both methods appear to direct the agent to avoid the cliff by staying above it until the goal is within reach.\n",
    "Differences:\n",
    "Value Iteration converges faster, meaning the optimal policy is reached in fewer iterations.\n",
    "Policy Iteration requires significantly more iterations, but the resulting state values and policy are nearly identical to those produced by Value Iteration.\n",
    "\n",
    "\n",
    "3. Experiment with different values of `gamma` and observe how it affects convergence and the optimal policy.\n",
    "\n",
    "Answer:\n",
    "Gamma = 0.1: Value Iteration converges after 7 iterations, reflecting a focus on short-term rewards, which leads to faster convergence but potentially riskier paths.\n",
    "Gamma = 0.5, 0.9, and 0.99: Value Iteration converges after 15 iterations. These higher Gamma values emphasize long-term rewards, making the agent consider future penalties more carefully, which takes longer to evaluate.\n",
    "\n",
    "Lower Gamma (0.1): Faster convergence, short-term focused policy.\n",
    "Higher Gamma (0.9, 0.99): Slower convergence, long-term focused policy.\n",
    "\n",
    "\n",
    "1. If the grids become \"slippery,\" where actions might lead to unintended moves, how would the optimal policy change?\n",
    "\n",
    "Answer:\n",
    "In a \"slippery\" environment:\n",
    "\n",
    "The agent would need to adopt a more conservative policy, avoiding routes near the cliff to minimize the risk of falling.\n",
    "The optimal path would likely involve staying farther from the cliff and taking safer routes, even if they are longer.\n",
    "The agent would prioritize actions that reduce the probability of slipping into the cliff, resulting in a more cautious and robust policy.\n",
    "\n",
    "\n",
    "5. If falling into the cliff results in a reward of -100, why is the state value not exactly -100?\n",
    "\n",
    "Answer:\n",
    "The state value is not exactly -100 because:\n",
    "\n",
    "The state value reflects the expected cumulative reward over time, considering the probabilities of future actions and outcomes.\n",
    "Even though the cliff penalty is -100, the agent's policy may avoid the cliff most of the time, leading to a weighted average of penalties and rewards.\n",
    "The state value considers the possibility of recovering from mistakes and reaching the goal, reducing the overall negative impact of the cliff penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Y0glhCqa63X",
   "metadata": {
    "id": "6Y0glhCqa63X"
   },
   "source": [
    "# Part 2: Reinforcement Learning\n",
    "\n",
    "In this part, you will implement two reinforcement learning algorithms, Q-learning and SARSA, for Blackjack.\n",
    "\n",
    "Blackjack, or 21, is a casino card game where players aim to beat the dealer by having a hand value closer to 21 without exceeding it. Cards 2-10 are worth their face value, face cards are 10, and Aces are 1 or 11 (whichever is more beneficial). Players can \"hit\" for more cards or \"stand\" to keep their total, while the dealer follows fixed rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rRK4k_JJbuhz",
   "metadata": {
    "id": "rRK4k_JJbuhz"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Let's create the game environment first. In Gymnasium's Blackjack environment, the cards are drawn from an infinite deck (i.e. drawn with replacement). This means that exactly which cards are drawn is not important except an ace (since it can have two values); only the total values are enough.\n",
    "\n",
    "Below, you can see that the observation space size is 32 \\* 11 \\* 2, where 32 is the number of total values the player can have over the course of the game, 11 is the numbers dealer's value can take, and 2 is whether the player has an usable ace or not. The player has only 2 actions, hit (1) or stand (0). A reward of 1 or -1 are given when the player wins or loses, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-fcLOLWMaggJ",
   "metadata": {
    "id": "-fcLOLWMaggJ"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=False, sab=False, render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "env.render()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1DjT7KWIdMMs",
   "metadata": {
    "id": "1DjT7KWIdMMs"
   },
   "source": [
    "## Task 4: Q-learning\n",
    "\n",
    "In class, you have learned Q-learning, which uses the following updates to learn Q-values:\n",
    "\n",
    "$Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t))$\n",
    "\n",
    "In the function below, implement the Q-learning algorithm with $\\epsilon$-greedy exploration with exponential decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kBqTv7csbIGE",
   "metadata": {
    "id": "kBqTv7csbIGE"
   },
   "outputs": [],
   "source": [
    "def q_learning(\n",
    "    env,\n",
    "    num_episodes,\n",
    "    gamma=0.9,\n",
    "    alpha=0.01,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=0.995,\n",
    "):\n",
    "    # Initialize Q-table\n",
    "    q_table = np.zeros((32, 11, 2, env.action_space.n))\n",
    "    policy_table = np.zeros((32, 11, 2))\n",
    "\n",
    "    epsilon = epsilon_start  # Initialize epsilon value\n",
    "\n",
    "    \"\"\"\n",
    "    Learn Q-values and the policy using Q-learning update rule.\n",
    "\n",
    "    Inputs:\n",
    "        - num_episodes: Number of episodes to run the algorithm for.\n",
    "        - gamma: Discount factor for future rewards.\n",
    "        - alpha: Learning rate for updating Q-values.\n",
    "        - epsilon_start: Initial value for epsilon in the epsilon-greedy policy.\n",
    "        - epsilon_min: Minimum value of epsilon.\n",
    "        - epsilon_decay: Decay factor for epsilon.\n",
    "\n",
    "    Returns:\n",
    "        - q_table: Q-values for each state-action (or observation-action) pair.\n",
    "        - policy_table: Optimal policy with respect to the learned Q-values.\n",
    "    \"\"\"\n",
    "\n",
    "    # START CODE HERE\n",
    "\n",
    "    # END CODE HERE\n",
    "\n",
    "    return q_table, policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QcKiwqSCgrqi",
   "metadata": {
    "id": "QcKiwqSCgrqi"
   },
   "source": [
    "# Visualizing the results\n",
    "\n",
    "Now we can visualize what the algorithm has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zA_RshSr0RK",
   "metadata": {
    "id": "7zA_RshSr0RK"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(q_table, policy_table):\n",
    "    \"\"\"\n",
    "    Evaluate the value of each state under a given policy.\n",
    "\n",
    "    Parameters:\n",
    "    - q_table: The Q-table learned from Q-learning (shape: [player_sum, dealer_card, usable_ace, action]).\n",
    "    - policy_table: The policy table derived from the Q-table (shape: [player_sum, dealer_card, usable_ace]).\n",
    "\n",
    "    Returns:\n",
    "    - value_table: The value of each state under the given policy (shape: [player_sum, dealer_card, usable_ace]).\n",
    "    \"\"\"\n",
    "    # Initialize the value table\n",
    "    value_table = np.zeros(policy_table.shape)\n",
    "\n",
    "    # Iterate over all possible states\n",
    "    for player_sum in range(32):\n",
    "        for dealer_card in range(11):\n",
    "            for usable_ace in range(2):\n",
    "                # Get the action chosen by the policy for this state\n",
    "                action = policy_table[player_sum, dealer_card, usable_ace]\n",
    "\n",
    "                # Get the Q-value for the chosen action from q_table\n",
    "                value_table[player_sum, dealer_card, usable_ace] = q_table[\n",
    "                    player_sum, dealer_card, usable_ace, action\n",
    "                ]\n",
    "\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z1QXkO1jmZVT",
   "metadata": {
    "id": "z1QXkO1jmZVT"
   },
   "outputs": [],
   "source": [
    "def plot_value_surface(value_table, usable_ace):\n",
    "    \"\"\"\n",
    "    Plot the 3D surface for the value table under a given condition (usable ace or not).\n",
    "    Only considers player sums >= 12.\n",
    "\n",
    "    Parameters:\n",
    "    - value_table: The value table computed from the policy.\n",
    "    - usable_ace: Boolean, whether to visualize values for states with a usable ace (True) or without (False).\n",
    "    \"\"\"\n",
    "    ace_index = 1 if usable_ace else 0\n",
    "    # Focus on valid player sums (12-21) and dealer card values (1-10)\n",
    "    player_sums = np.arange(12, 22)  # Player sum values: 12 to 21\n",
    "    dealer_cards = np.arange(1, 11)  # Dealer card values: 1 to 10\n",
    "\n",
    "    # Prepare the meshgrid for plotting\n",
    "    player_sum_grid, dealer_card_grid = np.meshgrid(player_sums, dealer_cards)\n",
    "\n",
    "    # Extract the value table slice for usable ace or not (only for player sums >= 12)\n",
    "    z_values = value_table[12:22, 1:11, ace_index]  # (10, 10) shape\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Transpose z_values to match the meshgrid dimensions\n",
    "    ax.plot_surface(dealer_card_grid, player_sum_grid, z_values.T, cmap=\"viridis\")\n",
    "\n",
    "    # Labels and title\n",
    "    title = (\n",
    "        \"Value Surface (Usable Ace)\" if usable_ace else \"Value Surface (No Usable Ace)\"\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Player Sum\")\n",
    "    ax.set_xlabel(\"Dealer Card\")\n",
    "    ax.set_zlabel(\"State Value\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_policy_heatmap(policy_table, usable_ace):\n",
    "    \"\"\"\n",
    "    Plot the policy heatmap for usable ace and non-usable ace conditions.\n",
    "    Only considers player sums >= 12.\n",
    "\n",
    "    Parameters:\n",
    "    - policy_table: The policy table computed from Q-learning.\n",
    "    - usable_ace: Boolean, whether to visualize for states with usable ace (True) or not (False).\n",
    "    \"\"\"\n",
    "    ace_index = 1 if usable_ace else 0\n",
    "    # Focus on valid player sums (12-21) and dealer card values (1-10)\n",
    "    player_sums = np.arange(12, 22)  # Player sum values: 12 to 21\n",
    "    dealer_cards = np.arange(1, 11)  # Dealer card values: 1 to 10\n",
    "\n",
    "    # Extract the policy table slice for usable ace or not (only for player sums >= 12)\n",
    "    policy_values = policy_table[12:22, 1:11, ace_index]  # (10, 10) shape\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        policy_values,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"coolwarm\",\n",
    "        xticklabels=dealer_cards,\n",
    "        yticklabels=player_sums,\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    # Labels and title\n",
    "    title = (\n",
    "        \"Policy Heatmap (Usable Ace)\"\n",
    "        if usable_ace\n",
    "        else \"Policy Heatmap (No Usable Ace)\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dealer Card\")\n",
    "    plt.ylabel(\"Player Sum\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FvjTALoM0uiF",
   "metadata": {
    "id": "FvjTALoM0uiF"
   },
   "outputs": [],
   "source": [
    "q_table, policy_table = q_learning(env, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FKpl5AltcE1y",
   "metadata": {
    "id": "FKpl5AltcE1y"
   },
   "outputs": [],
   "source": [
    "value_table = evaluate_policy(q_table, policy_table)\n",
    "\n",
    "plot_value_surface(value_table, usable_ace=True)\n",
    "plot_value_surface(value_table, usable_ace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zZYCSaJbwU-C",
   "metadata": {
    "id": "zZYCSaJbwU-C"
   },
   "outputs": [],
   "source": [
    "plot_policy_heatmap(policy_table, usable_ace=True)\n",
    "plot_policy_heatmap(policy_table, usable_ace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TI6VtOX4g81l",
   "metadata": {
    "id": "TI6VtOX4g81l"
   },
   "source": [
    "# Task 5: Analysis of the results\n",
    "\n",
    "Briefly answer the questions below. You may write your answers in this cell right after \"Answer:\".\n",
    "\n",
    "1. Is Q-learning on-policy or off-policy? Why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. What can you infer from the visualization of the value function (using the default hyperparameters for training)? Does it match your intuition?\n",
    "\n",
    "Answer:\n",
    "\n",
    "3. Intuitively, does the policy seem to be optimal (using the default hyperparameters for training)? Why or why not?\n",
    "\n",
    "Answer:\n",
    "\n",
    "4. For both value and policy visualizations, what difference does having or not having an usable ace make?\n",
    "\n",
    "Answer:\n",
    "\n",
    "5. Experiment with different values of $\\gamma, \\epsilon_{start}, \\epsilon_{min}$, $\\epsilon$ decay factor (you should also try a decay of 1, which corresponds to vanilla $\\epsilon$-greedy with $\\epsilon=\\epsilon_{start}$). How do these hyperparameters affect your results?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wipjZpQrhzEO",
   "metadata": {
    "id": "wipjZpQrhzEO"
   },
   "source": [
    "# Task 6: SARSA\n",
    "SARSA is another popular reinforcement learning algorithm. It uses the following update rule:\n",
    "\n",
    "$Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha (r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$\n",
    "\n",
    "where $a_{t+1}$ is also selected using the $\\epsilon$-greedy policy that is used to select $a_t$.\n",
    "\n",
    "Now, implement SARSA with $\\epsilon$-greedy exploration with decay in the function body below, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K_gNo-SmxbY1",
   "metadata": {
    "id": "K_gNo-SmxbY1"
   },
   "outputs": [],
   "source": [
    "def sarsa(\n",
    "    env,\n",
    "    num_episodes,\n",
    "    gamma=0.9,\n",
    "    alpha=0.01,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=0.995,\n",
    "):\n",
    "\n",
    "    # Initialize Q-table\n",
    "\n",
    "    q_table = np.zeros((32, 11, 2, env.action_space.n))\n",
    "\n",
    "    policy_table = np.zeros((32, 11, 2))\n",
    "\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Learn Q-values and the policy using SARSA update rule.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - num_episodes: Number of episodes to run the algorithm for.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - gamma: Discount factor for future rewards.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - alpha: Learning rate for updating Q-values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - epsilon_start: Initial value for epsilon in the epsilon-greedy policy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - epsilon_min: Minimum value of epsilon.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - epsilon_decay: Decay factor for epsilon.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - q_table: Q-values for each state-action (or observation-action) pair.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        - policy_table: Optimal policy with respect to the learned Q-values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # START CODE HERE\n",
    "\n",
    "\n",
    "    # END CODE HERE\n",
    "\n",
    "\n",
    "    return q_table, policy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ImwDGHby2Req",
   "metadata": {
    "id": "ImwDGHby2Req"
   },
   "outputs": [],
   "source": [
    "q_table, policy_table = sarsa(env, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMXS1bbIi5hT",
   "metadata": {
    "id": "YMXS1bbIi5hT"
   },
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jQcZvf3N2VC1",
   "metadata": {
    "id": "jQcZvf3N2VC1"
   },
   "outputs": [],
   "source": [
    "value_table = evaluate_policy(q_table, policy_table)\n",
    "\n",
    "plot_value_surface(value_table, usable_ace=True)\n",
    "plot_value_surface(value_table, usable_ace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4YVo2_5A2jLx",
   "metadata": {
    "id": "4YVo2_5A2jLx"
   },
   "outputs": [],
   "source": [
    "plot_policy_heatmap(policy_table, usable_ace=True)\n",
    "plot_policy_heatmap(policy_table, usable_ace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XPe_OUSi8yX",
   "metadata": {
    "id": "3XPe_OUSi8yX"
   },
   "source": [
    "# Task 7: Analysis of results\n",
    "\n",
    "Briefly answer the questions below.\n",
    "\n",
    "1. Is SARSA on-policy or off-policy? Why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. Experiment with different values of $\\gamma, \\epsilon_{start}, \\epsilon_{min}$, $\\epsilon$ decay factor (you should also try a decay of 1, which corresponds to vanilla $\\epsilon$-greedy with $\\epsilon=\\epsilon_{start}$). How do these hyperparameters affect your results?\n",
    "\n",
    "Answer:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
