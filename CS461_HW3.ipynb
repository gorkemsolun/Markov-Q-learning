{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS461 Homework Assignment 3"
      ],
      "metadata": {
        "id": "JOVOX-VhDKvS"
      },
      "id": "JOVOX-VhDKvS"
    },
    {
      "cell_type": "markdown",
      "id": "02a7b628",
      "metadata": {
        "id": "02a7b628"
      },
      "source": [
        "# Part 1: Solving Cliff Walking with Value Iteration and Policy Iteration\n",
        "In this part, you will implement **Value Iteration** and **Policy Iteration** methods to solve the `CliffWalking` environment from the Gymnasium library. The goal is to find an optimal policy that minimizes the total cost while avoiding the cliff.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand and implement core MDP methods (Value Iteration and Policy Iteration).\n",
        "- Visualize and analyze the convergence of state values and policies.\n",
        "- Compare the performance of both methods in terms of iterations and computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7682044",
      "metadata": {
        "id": "c7682044"
      },
      "source": [
        "## Setup\n",
        "### Install Required Libraries\n",
        "To begin, ensure you have the required packages installed. Run the following cells to install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4vhCexoicqFr",
      "metadata": {
        "id": "4vhCexoicqFr"
      },
      "outputs": [],
      "source": [
        "!pip install swig --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5f06f8",
      "metadata": {
        "id": "5b5f06f8"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[all] matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3644ef",
      "metadata": {
        "id": "1b3644ef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium.envs.toy_text import CliffWalkingEnv\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from matplotlib.patches import Patch\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bbb723",
      "metadata": {
        "id": "30bbb723"
      },
      "source": [
        "## Environment\n",
        "The `CliffWalkingEnv` is a 4x12 grid-world environment. The agent starts at the top-left corner and aims to reach the top-right corner. Falling into the cliff results in a significant penalty. The environment has four possible actions:\n",
        "- **0**: Move up\n",
        "- **1**: Move right\n",
        "- **2**: Move down\n",
        "- **3**: Move left\n",
        "\n",
        "### Example Render:\n",
        "S = Start, G = Goal, C = Cliff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9yA6Si8iGCd",
      "metadata": {
        "id": "Y9yA6Si8iGCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gymnasium import Wrapper\n",
        "\n",
        "class CliffEnvWrapper(Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.start_state = (3, 0)  # Assumes a fixed start state\n",
        "        self.terminal_state = (self.env.shape[0] - 1, self.env.shape[1] - 1)  # Bottom-right corner\n",
        "        self.start_state_index = np.ravel_multi_index(self.start_state, self.env.shape)\n",
        "        self.terminal_state_index = np.ravel_multi_index(self.terminal_state, self.env.shape)\n",
        "\n",
        "        # Expose `P` as a direct attribute\n",
        "        self.P = env.P\n",
        "\n",
        "        # Correct the transition probabilities\n",
        "        self.correct_transitions()\n",
        "\n",
        "    def correct_transitions(self):\n",
        "        \"\"\"Corrects the environment's transition probabilities for cliffs and terminal states.\"\"\"\n",
        "        for state in range(self.env.nS):\n",
        "            position = np.unravel_index(state, self.env.shape)\n",
        "\n",
        "            for action in range(self.env.nA):\n",
        "                transitions = self.env.P[state][action]\n",
        "\n",
        "                # If the state is a cliff, reset to the start state with penalty\n",
        "                if self.env._cliff[position]:\n",
        "                    self.env.P[state][action] = [(1.0, self.start_state_index, -100, False)]\n",
        "                    continue\n",
        "\n",
        "                # If the state is terminal, transitions should only lead to itself\n",
        "                if position == self.terminal_state:\n",
        "                    self.env.P[state][action] = [(1.0, state, 0, True)]\n",
        "                    continue\n",
        "\n",
        "                # Otherwise, keep the original transition logic\n",
        "                new_transitions = []\n",
        "                for prob, next_state, reward, done in transitions:\n",
        "                    next_position = np.unravel_index(next_state, self.env.shape)\n",
        "                    # Ensure terminal state is correctly handled\n",
        "                    if next_position == self.terminal_state:\n",
        "                        new_transitions.append((prob, next_state, reward, True))\n",
        "                    else:\n",
        "                        new_transitions.append((prob, next_state, reward, False))\n",
        "                self.env.P[state][action] = new_transitions\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Override step to ensure consistency with wrapped transitions.\"\"\"\n",
        "        state, reward, done, truncated, info = super().step(action)\n",
        "\n",
        "        # If the agent steps into a cliff, reset it to the start state\n",
        "        position = np.unravel_index(self.env.s, self.env.shape)\n",
        "        if self.env._cliff[position]:\n",
        "            self.env.s = self.start_state_index  # Reset to start state\n",
        "            return self.env.s, reward, done, truncated, info\n",
        "\n",
        "        return state, reward, done, truncated, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049ea1c5",
      "metadata": {
        "id": "049ea1c5"
      },
      "outputs": [],
      "source": [
        "env = CliffWalkingEnv(render_mode=\"rgb_array\")\n",
        "wrapped_env = CliffEnvWrapper(env)\n",
        "\n",
        "# Use the wrapped environment\n",
        "observation, info = wrapped_env.reset()\n",
        "wrapped_env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a50169",
      "metadata": {
        "id": "b7a50169"
      },
      "source": [
        "## Task 1: Implement Value Iteration\n",
        "Complete the function `value_iteration` below. This method computes the optimal state-value function and policy by iteratively updating state values using the Bellman optimality equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487c2c2e",
      "metadata": {
        "id": "487c2c2e"
      },
      "outputs": [],
      "source": [
        "# Value Iteration function skeleton\n",
        "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
        "    \"\"\"\n",
        "    Performs Value Iteration to compute the optimal value function and policy.\n",
        "    Args:\n",
        "        env: The CliffWalking environment.\n",
        "        gamma: Discount factor.\n",
        "        theta: Threshold for convergence.\n",
        "    Returns:\n",
        "        V: Optimal state-value function.\n",
        "        policy: Optimal policy.\n",
        "    \"\"\"\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # TODO: Update state values using Bellman optimality equation\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    # TODO: Derive optimal policy from the value function\n",
        "\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03404f5",
      "metadata": {
        "id": "e03404f5"
      },
      "source": [
        "## Task 2: Implement Policy Iteration\n",
        "Complete the function `policy_iteration` below. This method alternates between policy evaluation and policy improvement until the policy converges to the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a99b06d",
      "metadata": {
        "id": "2a99b06d"
      },
      "outputs": [],
      "source": [
        "# Policy Iteration function skeleton\n",
        "def policy_iteration(env, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Performs Policy Iteration to compute the optimal value function and policy.\n",
        "    Args:\n",
        "        env: The CliffWalking environment.\n",
        "        gamma: Discount factor.\n",
        "    Returns:\n",
        "        V: Optimal state-value function.\n",
        "        policy: Optimal policy.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "\n",
        "    while True:\n",
        "        # TODO: Perform policy evaluation and policy improvement\n",
        "        break # replace with your actual algorithm\n",
        "        # Check for policy convergence\n",
        "\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3603906f",
      "metadata": {
        "id": "3603906f"
      },
      "source": [
        "## Visualization\n",
        "To help you debug and understand the algorithms, use the following function to visualize the state values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6631e053",
      "metadata": {
        "id": "6631e053"
      },
      "outputs": [],
      "source": [
        "def visualize_values(values, grid_size=(4, 12)):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    grid_values = np.array(values).reshape(grid_size)\n",
        "    plt.imshow(grid_values, cmap='coolwarm', interpolation='nearest')\n",
        "    for (i, j), val in np.ndenumerate(grid_values):\n",
        "        plt.text(j, i, f\"{val:.2f}\", ha='center', va='center', color='black')\n",
        "    plt.title(\"State Values\")\n",
        "    plt.colorbar(label=\"Value\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279e724d",
      "metadata": {
        "id": "279e724d"
      },
      "outputs": [],
      "source": [
        "# Test the solutions and visualize results\n",
        "# Value Iteration\n",
        "\n",
        "wrapped_env.reset()\n",
        "V_vi, policy_vi = value_iteration(wrapped_env)\n",
        "visualize_values(V_vi)\n",
        "\n",
        "\n",
        "# Policy Iteration\n",
        "V_pi, policy_pi = policy_iteration(env)\n",
        "visualize_values(V_pi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1e80f7",
      "metadata": {
        "id": "8b1e80f7"
      },
      "source": [
        "## Task 3: Analysis\n",
        "\n",
        "Briefly answer the questions below. You may write your answers after \"Answer:\".\n",
        "\n",
        "1. Compare the number of iterations and computation time for Value Iteration and Policy Iteration.\n",
        "\n",
        "Answer:\n",
        "\n",
        "2. Visualize the state values and return the optimal policy for both methods and discuss any differences.\n",
        "\n",
        "Answer:\n",
        "\n",
        "3. Experiment with different values of `gamma` and observe how it affects convergence and the optimal policy.\n",
        "\n",
        "Answer:\n",
        "\n",
        "4. If the grids become \"slippery,\" where actions might lead to unintended moves, how would the optimal policy change?\n",
        "\n",
        "Answer:\n",
        "\n",
        "5. If falling into the cliff results in a reward of -100, why is the state value not exactly -100?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Reinforcement Learning\n",
        "\n",
        "In this part, you will implement two reinforcement learning algorithms, Q-learning and SARSA, for Blackjack.\n",
        "\n",
        "Blackjack, or 21, is a casino card game where players aim to beat the dealer by having a hand value closer to 21 without exceeding it. Cards 2-10 are worth their face value, face cards are 10, and Aces are 1 or 11 (whichever is more beneficial). Players can \"hit\" for more cards or \"stand\" to keep their total, while the dealer follows fixed rules."
      ],
      "metadata": {
        "id": "6Y0glhCqa63X"
      },
      "id": "6Y0glhCqa63X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Let's create the game environment first. In Gymnasium's Blackjack environment, the cards are drawn from an infinite deck (i.e. drawn with replacement). This means that exactly which cards are drawn is not important except an ace (since it can have two values); only the total values are enough.\n",
        "\n",
        "Below, you can see that the observation space size is 32 \\* 11 \\* 2, where 32 is the number of total values the player can have over the course of the game, 11 is the numbers dealer's value can take, and 2 is whether the player has an usable ace or not. The player has only 2 actions, hit (1) or stand (0). A reward of 1 or -1 are given when the player wins or loses, respectively."
      ],
      "metadata": {
        "id": "rRK4k_JJbuhz"
      },
      "id": "rRK4k_JJbuhz"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v1', natural=False, sab=False, render_mode=\"rgb_array\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "\n",
        "env.render()\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "metadata": {
        "id": "-fcLOLWMaggJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-fcLOLWMaggJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Q-learning\n",
        "\n",
        "In class, you have learned Q-learning, which uses the following updates to learn Q-values:\n",
        "\n",
        "$Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t))$\n",
        "\n",
        "In the function below, implement the Q-learning algorithm with $\\epsilon$-greedy exploration with exponential decay."
      ],
      "metadata": {
        "id": "1DjT7KWIdMMs"
      },
      "id": "1DjT7KWIdMMs"
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes, gamma=0.9, alpha=0.01, epsilon_start=1.0, epsilon_min=0.1, epsilon_decay=0.995):\n",
        "    # Initialize Q-table\n",
        "    q_table = np.zeros((32, 11, 2, env.action_space.n))\n",
        "    policy_table = np.zeros((32, 11, 2))\n",
        "\n",
        "    epsilon = epsilon_start  # Initialize epsilon value\n",
        "\n",
        "    \"\"\"\n",
        "    Learn Q-values and the policy using Q-learning update rule.\n",
        "\n",
        "    Inputs:\n",
        "        - num_episodes: Number of episodes to run the algorithm for.\n",
        "        - gamma: Discount factor for future rewards.\n",
        "        - alpha: Learning rate for updating Q-values.\n",
        "        - epsilon_start: Initial value for epsilon in the epsilon-greedy policy.\n",
        "        - epsilon_min: Minimum value of epsilon.\n",
        "        - epsilon_decay: Decay factor for epsilon.\n",
        "\n",
        "    Returns:\n",
        "        - q_table: Q-values for each state-action (or observation-action) pair.\n",
        "        - policy_table: Optimal policy with respect to the learned Q-values.\n",
        "    \"\"\"\n",
        "\n",
        "    # START CODE HERE\n",
        "\n",
        "    # END CODE HERE\n",
        "\n",
        "    return q_table, policy_table\n"
      ],
      "metadata": {
        "id": "kBqTv7csbIGE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kBqTv7csbIGE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the results\n",
        "\n",
        "Now we can visualize what the algorithm has learned."
      ],
      "metadata": {
        "id": "QcKiwqSCgrqi"
      },
      "id": "QcKiwqSCgrqi"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(q_table, policy_table):\n",
        "    \"\"\"\n",
        "    Evaluate the value of each state under a given policy.\n",
        "\n",
        "    Parameters:\n",
        "    - q_table: The Q-table learned from Q-learning (shape: [player_sum, dealer_card, usable_ace, action]).\n",
        "    - policy_table: The policy table derived from the Q-table (shape: [player_sum, dealer_card, usable_ace]).\n",
        "\n",
        "    Returns:\n",
        "    - value_table: The value of each state under the given policy (shape: [player_sum, dealer_card, usable_ace]).\n",
        "    \"\"\"\n",
        "    # Initialize the value table\n",
        "    value_table = np.zeros(policy_table.shape)\n",
        "\n",
        "    # Iterate over all possible states\n",
        "    for player_sum in range(32):\n",
        "        for dealer_card in range(11):\n",
        "            for usable_ace in range(2):\n",
        "                # Get the action chosen by the policy for this state\n",
        "                action = policy_table[player_sum, dealer_card, usable_ace]\n",
        "\n",
        "                # Get the Q-value for the chosen action from q_table\n",
        "                value_table[player_sum, dealer_card, usable_ace] = q_table[player_sum, dealer_card, usable_ace, action]\n",
        "\n",
        "    return value_table\n"
      ],
      "metadata": {
        "id": "7zA_RshSr0RK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7zA_RshSr0RK"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_surface(value_table, usable_ace):\n",
        "    \"\"\"\n",
        "    Plot the 3D surface for the value table under a given condition (usable ace or not).\n",
        "    Only considers player sums >= 12.\n",
        "\n",
        "    Parameters:\n",
        "    - value_table: The value table computed from the policy.\n",
        "    - usable_ace: Boolean, whether to visualize values for states with a usable ace (True) or without (False).\n",
        "    \"\"\"\n",
        "    ace_index = 1 if usable_ace else 0\n",
        "    # Focus on valid player sums (12-21) and dealer card values (1-10)\n",
        "    player_sums = np.arange(12, 22)  # Player sum values: 12 to 21\n",
        "    dealer_cards = np.arange(1, 11)  # Dealer card values: 1 to 10\n",
        "\n",
        "    # Prepare the meshgrid for plotting\n",
        "    player_sum_grid, dealer_card_grid = np.meshgrid(player_sums, dealer_cards)\n",
        "\n",
        "    # Extract the value table slice for usable ace or not (only for player sums >= 12)\n",
        "    z_values = value_table[12:22, 1:11, ace_index]  # (10, 10) shape\n",
        "\n",
        "    # Plotting\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Transpose z_values to match the meshgrid dimensions\n",
        "    ax.plot_surface(dealer_card_grid, player_sum_grid, z_values.T, cmap='viridis')\n",
        "\n",
        "    # Labels and title\n",
        "    title = \"Value Surface (Usable Ace)\" if usable_ace else \"Value Surface (No Usable Ace)\"\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel('Player Sum')\n",
        "    ax.set_xlabel('Dealer Card')\n",
        "    ax.set_zlabel('State Value')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_policy_heatmap(policy_table, usable_ace):\n",
        "    \"\"\"\n",
        "    Plot the policy heatmap for usable ace and non-usable ace conditions.\n",
        "    Only considers player sums >= 12.\n",
        "\n",
        "    Parameters:\n",
        "    - policy_table: The policy table computed from Q-learning.\n",
        "    - usable_ace: Boolean, whether to visualize for states with usable ace (True) or not (False).\n",
        "    \"\"\"\n",
        "    ace_index = 1 if usable_ace else 0\n",
        "    # Focus on valid player sums (12-21) and dealer card values (1-10)\n",
        "    player_sums = np.arange(12, 22)  # Player sum values: 12 to 21\n",
        "    dealer_cards = np.arange(1, 11)  # Dealer card values: 1 to 10\n",
        "\n",
        "    # Extract the policy table slice for usable ace or not (only for player sums >= 12)\n",
        "    policy_values = policy_table[12:22, 1:11, ace_index]  # (10, 10) shape\n",
        "\n",
        "    # Plotting the heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(policy_values, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=dealer_cards, yticklabels=player_sums, cbar=False)\n",
        "\n",
        "    # Labels and title\n",
        "    title = \"Policy Heatmap (Usable Ace)\" if usable_ace else \"Policy Heatmap (No Usable Ace)\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Dealer Card')\n",
        "    plt.ylabel('Player Sum')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "z1QXkO1jmZVT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "z1QXkO1jmZVT"
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, policy_table = q_learning(env, num_episodes=100000)"
      ],
      "metadata": {
        "id": "FvjTALoM0uiF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FvjTALoM0uiF"
    },
    {
      "cell_type": "code",
      "source": [
        "value_table = evaluate_policy(q_table, policy_table)\n",
        "\n",
        "plot_value_surface(value_table, usable_ace=True)\n",
        "plot_value_surface(value_table, usable_ace=False)"
      ],
      "metadata": {
        "id": "FKpl5AltcE1y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FKpl5AltcE1y"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_policy_heatmap(policy_table, usable_ace=True)\n",
        "plot_policy_heatmap(policy_table, usable_ace=False)"
      ],
      "metadata": {
        "id": "zZYCSaJbwU-C"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zZYCSaJbwU-C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Analysis of the results\n",
        "\n",
        "Briefly answer the questions below. You may write your answers in this cell right after \"Answer:\".\n",
        "\n",
        "1. Is Q-learning on-policy or off-policy? Why?\n",
        "\n",
        "Answer:\n",
        "\n",
        "2. What can you infer from the visualization of the value function (using the default hyperparameters for training)? Does it match your intuition?\n",
        "\n",
        "Answer:\n",
        "\n",
        "3. Intuitively, does the policy seem to be optimal (using the default hyperparameters for training)? Why or why not?\n",
        "\n",
        "Answer:\n",
        "\n",
        "4. For both value and policy visualizations, what difference does having or not having an usable ace make?\n",
        "\n",
        "Answer:\n",
        "\n",
        "5. Experiment with different values of $\\gamma, \\epsilon_{start}, \\epsilon_{min}$, $\\epsilon$ decay factor (you should also try a decay of 1, which corresponds to vanilla $\\epsilon$-greedy with $\\epsilon=\\epsilon_{start}$). How do these hyperparameters affect your results?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "TI6VtOX4g81l"
      },
      "id": "TI6VtOX4g81l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: SARSA\n",
        "SARSA is another popular reinforcement learning algorithm. It uses the following update rule:\n",
        "\n",
        "$Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha (r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$\n",
        "\n",
        "where $a_{t+1}$ is also selected using the $\\epsilon$-greedy policy that is used to select $a_t$.\n",
        "\n",
        "Now, implement SARSA with $\\epsilon$-greedy exploration with decay in the function body below, as before."
      ],
      "metadata": {
        "id": "wipjZpQrhzEO"
      },
      "id": "wipjZpQrhzEO"
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, num_episodes, gamma=0.9, alpha=0.01, epsilon_start=1.0, epsilon_min=0.1, epsilon_decay=0.995):\n",
        "    # Initialize Q-table\n",
        "    q_table = np.zeros((32, 11, 2, env.action_space.n))\n",
        "    policy_table = np.zeros((32, 11, 2))\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    \"\"\"\n",
        "    Learn Q-values and the policy using SARSA update rule.\n",
        "\n",
        "    Inputs:\n",
        "        - num_episodes: Number of episodes to run the algorithm for.\n",
        "        - gamma: Discount factor for future rewards.\n",
        "        - alpha: Learning rate for updating Q-values.\n",
        "        - epsilon_start: Initial value for epsilon in the epsilon-greedy policy.\n",
        "        - epsilon_min: Minimum value of epsilon.\n",
        "        - epsilon_decay: Decay factor for epsilon.\n",
        "\n",
        "    Returns:\n",
        "        - q_table: Q-values for each state-action (or observation-action) pair.\n",
        "        - policy_table: Optimal policy with respect to the learned Q-values.\n",
        "    \"\"\"\n",
        "    # START CODE HERE\n",
        "\n",
        "    # END CODE HERE\n",
        "\n",
        "    return q_table, policy_table\n"
      ],
      "metadata": {
        "id": "K_gNo-SmxbY1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "K_gNo-SmxbY1"
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, policy_table = sarsa(env, num_episodes=100000)"
      ],
      "metadata": {
        "id": "ImwDGHby2Req"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ImwDGHby2Req"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the results"
      ],
      "metadata": {
        "id": "YMXS1bbIi5hT"
      },
      "id": "YMXS1bbIi5hT"
    },
    {
      "cell_type": "code",
      "source": [
        "value_table = evaluate_policy(q_table, policy_table)\n",
        "\n",
        "plot_value_surface(value_table, usable_ace=True)\n",
        "plot_value_surface(value_table, usable_ace=False)"
      ],
      "metadata": {
        "id": "jQcZvf3N2VC1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jQcZvf3N2VC1"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_policy_heatmap(policy_table, usable_ace=True)\n",
        "plot_policy_heatmap(policy_table, usable_ace=False)"
      ],
      "metadata": {
        "id": "4YVo2_5A2jLx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4YVo2_5A2jLx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7: Analysis of results\n",
        "\n",
        "Briefly answer the questions below.\n",
        "\n",
        "1. Is SARSA on-policy or off-policy? Why?\n",
        "\n",
        "Answer:\n",
        "\n",
        "2. Experiment with different values of $\\gamma, \\epsilon_{start}, \\epsilon_{min}$, $\\epsilon$ decay factor (you should also try a decay of 1, which corresponds to vanilla $\\epsilon$-greedy with $\\epsilon=\\epsilon_{start}$). How do these hyperparameters affect your results?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "3XPe_OUSi8yX"
      },
      "id": "3XPe_OUSi8yX"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}